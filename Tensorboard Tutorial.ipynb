{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer1 = nn.Linear(16*7*7, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='Data/MNIST-Dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "writer = SummaryWriter(f'runs/MNIST/tensorboard_tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 64, 128, 1024]\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    loop_info = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for batch_idx, (data, targets) in loop_info:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        predicted_output = model(data)\n",
    "        loss = loss_fn(predicted_output, targets)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculating accuracy\n",
    "        _, predictions = predicted_output.max(1)\n",
    "        num_correct = (predictions == targets).sum()\n",
    "        running_acc = float(num_correct) / float(data.shape[0])\n",
    "        \n",
    "        # Writing to tensorboard\n",
    "        writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "        writer.add_scalar('Training Accuracy', running_acc, global_step=step)\n",
    "        step += 1\n",
    "        \n",
    "        loop_info.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        loop_info.set_postfix(acc=running_acc, loss=loss)\n",
    "print(\"Training Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and see results on tensorflow.\n",
    "# In anaconda cmd line, in current folder\n",
    "# tensorboard --logdir --runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "# Visualization 1\n",
    "# Graphs for Loss and Acc for diff batch_size and LR\n",
    "\n",
    "num_epochs = 1\n",
    "batch_sizes = [64, 1024]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "        # Just to reset params\n",
    "        model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        writer = SummaryWriter(f'runs/MNIST/experiment/BatchSize{batch_size} LR{learning_rate}')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            loop_info = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "            for batch_idx, (data, targets) in loop_info:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                predicted_output = model(data)\n",
    "                loss = loss_fn(predicted_output, targets)\n",
    "                losses.append(loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculating accuracy\n",
    "                _, predictions = predicted_output.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_acc = float(num_correct) / float(data.shape[0])\n",
    "\n",
    "                # Writing to tensorboard\n",
    "                writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "                writer.add_scalar('Training Accuracy', running_acc, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "                loop_info.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                loop_info.set_postfix(acc=running_acc, loss=loss)\n",
    "print(\"Training Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Experiment 2 \n",
    "# Visualization 2\n",
    "# Visualization for HPARAMS\n",
    "\n",
    "num_epochs = 1\n",
    "batch_sizes = [64, 1024]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "        # Just to reset params\n",
    "        model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        writer = SummaryWriter(f'runs/MNIST/experiment/BatchSize{batch_size} LR{learning_rate}')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            loop_info = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "            for batch_idx, (data, targets) in loop_info:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                predicted_output = model(data)\n",
    "                loss = loss_fn(predicted_output, targets)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculating accuracy\n",
    "                _, predictions = predicted_output.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_acc = float(num_correct) / float(data.shape[0])\n",
    "                accuracies.append(running_acc)\n",
    "                \n",
    "                # Writing to tensorboard\n",
    "                # Showing Training Loss and Accuracy over time\n",
    "                writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "                writer.add_scalar('Training Accuracy', running_acc, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "                loop_info.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                loop_info.set_postfix(acc=running_acc, loss=loss)\n",
    "            \n",
    "            # For Showing HPARAMS\n",
    "            writer.add_hparams(\n",
    "                hparam_dict = {\n",
    "                'lr': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "                },\n",
    "                metric_dict = {\n",
    "                'accuracy': sum(accuracies) / len(accuracies),\n",
    "                'loss': sum(losses) / len(losses)\n",
    "                })\n",
    "print(\"Training Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed!!\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 \n",
    "# Visualization 2\n",
    "# Visualization for HPARAMS\n",
    "\n",
    "num_epochs = 1\n",
    "batch_sizes = [64]\n",
    "learning_rates = [1e-4]\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "        # Just to reset params\n",
    "        model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        writer = SummaryWriter(f'runs/MNIST/experiment-viz/BatchSize{batch_size} LR{learning_rate}')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            loop_info = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "            for batch_idx, (data, targets) in loop_info:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                predicted_output = model(data)\n",
    "                loss = loss_fn(predicted_output, targets)\n",
    "                losses.append(loss)\n",
    "\n",
    "                # Showing image of each epoch\n",
    "                img_grid = torchvision.utils.make_grid(data)\n",
    "                writer.add_image('mninst_images', img_grid)\n",
    "                \n",
    "                # Seeing weights distribution of each batch\n",
    "                writer.add_histogram('layer1', model.layer1.weight)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculating accuracy\n",
    "                _, predictions = predicted_output.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_acc = float(num_correct) / float(data.shape[0])\n",
    "                accuracies.append(running_acc)\n",
    "                \n",
    "                # Writing to tensorboard\n",
    "                # Showing Training Loss and Accuracy over time\n",
    "                writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "                writer.add_scalar('Training Accuracy', running_acc, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "                loop_info.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                loop_info.set_postfix(acc=running_acc, loss=loss)\n",
    "            \n",
    "            # For Showing HPARAMS\n",
    "            writer.add_hparams(\n",
    "                hparam_dict = {\n",
    "                'lr': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "                },\n",
    "                metric_dict = {\n",
    "                'accuracy': sum(accuracies) / len(accuracies),\n",
    "                'loss': sum(losses) / len(losses)\n",
    "                })\n",
    "print(\"Training Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed!!\n"
     ]
    }
   ],
   "source": [
    "# Tensorboard embedding projection\n",
    "# To see how does the model evolve during training\n",
    "\n",
    "num_epochs = 1\n",
    "batch_sizes = [256]\n",
    "learning_rates = [1e-4]\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "        # Just to reset params\n",
    "        model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        writer = SummaryWriter(f'runs/MNIST/experiment-viz/BatchSize{batch_size} LR{learning_rate}')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            loop_info = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "            for batch_idx, (data, targets) in loop_info:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                predicted_output = model(data)\n",
    "                loss = loss_fn(predicted_output, targets)\n",
    "                losses.append(loss)\n",
    "\n",
    "                \n",
    "                # Showing image of each epoch\n",
    "                img_grid = torchvision.utils.make_grid(data)\n",
    "                writer.add_image('mninst_images', img_grid)\n",
    "                \n",
    "                # Seeing weights distribution of each batch\n",
    "                writer.add_histogram('layer1', model.layer1.weight)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculating accuracy\n",
    "                _, predictions = predicted_output.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_acc = float(num_correct) / float(data.shape[0])\n",
    "                accuracies.append(running_acc)\n",
    "                \n",
    "                 # Evolution during training\n",
    "                features = data.reshape(data.shape[0], -1)\n",
    "                class_labels = [classes[label] for label in predictions]\n",
    "                if (batch_idx==230):\n",
    "                    writer.add_embedding(features, metadata=class_labels, label_img=data)\n",
    "                \n",
    "                \n",
    "                # Writing to tensorboard\n",
    "                # Showing Training Loss and Accuracy over time\n",
    "                writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "                writer.add_scalar('Training Accuracy', running_acc, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "                loop_info.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                loop_info.set_postfix(acc=running_acc, loss=loss)\n",
    "            \n",
    "            # For Showing HPARAMS\n",
    "            writer.add_hparams(\n",
    "                hparam_dict = {\n",
    "                'lr': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "                },\n",
    "                metric_dict = {\n",
    "                'accuracy': sum(accuracies) / len(accuracies),\n",
    "                'loss': sum(losses) / len(losses)\n",
    "                })\n",
    "print(\"Training Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
